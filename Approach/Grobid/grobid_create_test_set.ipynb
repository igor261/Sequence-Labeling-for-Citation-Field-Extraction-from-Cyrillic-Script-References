{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def test_tokenize(text):\n",
    "    nltk_tokens = []\n",
    "    pattern = re.compile(\"[0-9]+\\.\")\n",
    "    pattern2= re.compile(\":[0-9]+\")\n",
    "    pattern3= re.compile(\"([A-Z]\\.)|([\\w?-?]\\.)\")\n",
    "    pattern4 = re.compile(\".*\\.\")\n",
    "\n",
    "    for t in nltk.word_tokenize(text,preserve_line=True):\n",
    "        if pattern2.match(t)!=None:\n",
    "            nltk_tokens.append(\":\")\n",
    "            if \".\" in t:\n",
    "                nltk_tokens.append(t.replace(\":\",\"\").replace(\".\",\"\"))\n",
    "            else:\n",
    "                nltk_tokens.append(t.replace(\":\",\"\"))\n",
    "            nltk_tokens.append(\".\")\n",
    "        elif ((pattern.match(t)!= None) or (pattern3.match(t)== None and pattern4.match(t)!=None)) and t !=\".\":\n",
    "            nltk_tokens.append(t.replace(\".\",\"\"))\n",
    "            nltk_tokens.append(\".\")\n",
    "\n",
    "        else:\n",
    "            nltk_tokens.append(t)\n",
    "    return(nltk_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_path(d):\n",
    "    # Return full path of all files & directories in directory\n",
    "    list_full_path = []\n",
    "    for path in os.listdir(d):\n",
    "        full_path = os.path.join(d, path)\n",
    "        list_full_path.append(full_path)\n",
    "    return list_full_path\n",
    "    \n",
    "# path = \"C:/Masterarbeit/Data/manuel_annotated/real_text4\" \n",
    "path = \".\\Sequence-Labeling-for-Reference-Parsing-of-Cyrillic-Script-Scholarly-Data\\Real_annotated_data\\labelled_text_per_paper\"\n",
    "files = listdir_path(path)\n",
    "print(\"Anzahl files: \" + str(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref = []\n",
    "file_ids = []\n",
    "for f in files:\n",
    "    file = open(f,\"r\",encoding=\"utf-8\")\n",
    "    ref = file.read().split(\"\\n\\n\")\n",
    "    for i in range(len(ref)):\n",
    "        file_id = f.split(\"\\\\\")[-1].replace(\".txt\",\"\")+f\"_{i}\"\n",
    "        file_ids.append(file_id)\n",
    "    all_ref+=ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame({\"Text\": [], \"Labels\": []})\n",
    "\n",
    "for ref in all_ref:\n",
    "    tokens = []\n",
    "    labels = []\n",
    "#     print(ref)\n",
    "    for f in ref.split(\"<\"):\n",
    "        if f == \"\":\n",
    "            pass\n",
    "        else:\n",
    "            text = f.split(\">\")\n",
    "            if len(text)>1:\n",
    "                text_token = test_tokenize(text[1])\n",
    "        #         new_text = \" \".join(test_tokenize(text[1]))\n",
    "        #         tokens+=test_tokenize(text[1])\n",
    "                if \"/\" in text[0]:\n",
    "                    for i in text_token:\n",
    "                        tokens.append(i)\n",
    "                        labels.append(\"other\")\n",
    "                else:\n",
    "                    label = text[0]\n",
    "                    for i in text_token:\n",
    "                        tokens.append(i)\n",
    "                        if label==\"author\" and i==\",\":\n",
    "                            labels.append(\"other\")\n",
    "                        else:\n",
    "                            labels.append(label)\n",
    "            else:\n",
    "                text_token = test_tokenize(text[0])\n",
    "                for i in text_token:\n",
    "                    tokens.append(i)\n",
    "                    labels.append(\"other\")\n",
    "    new_text = ' '.join(tokens)\n",
    "    new_labels = ' '.join(labels)\n",
    "\n",
    "        \n",
    "    df_append = pd.DataFrame({'Text' : [new_text] , 'Labels':[new_labels]})\n",
    "    df_data = df_data.append(df_append , ignore_index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = list(df_data.Text)\n",
    "# print(test_text[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = list(df_data.Labels)\n",
    "# print(test_labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_beg = {\"title\":\"\"\"<title level=\"a\">\"\"\" ,\n",
    "\"author\":\"\"\"<author>\"\"\",\n",
    "\"booktitle\":\"\"\"<title level=\"j\">\"\"\",\n",
    "\"journal\":\"\"\"<title level=\"j\">\"\"\",\n",
    "\"publisher\":\"\"\"<publisher>\"\"\",\n",
    "\"volume\":\"\"\"<biblScope unit=\"volume\">\"\"\",\n",
    "\"year\":\"\"\"<date>\"\"\",\n",
    "\"address\":\"\"\"<pubPlace>\"\"\",\n",
    "\"number\":\"\"\"<biblScope unit=\"issue\">\"\"\",\n",
    "\"pages\":\"\"\"<biblScope unit=\"page\">\"\"\",\n",
    "\"pagetotal\":\"\"\"<biblScope unit=\"page\">\"\"\"}\n",
    "\n",
    "label_dict_end = {\"title\":\"\"\"</title>\"\"\" ,\n",
    "\"author\":\"\"\"</author>\"\"\",\n",
    "\"booktitle\":\"\"\"</title>\"\"\",\n",
    "\"journal\":\"\"\"</title>\"\"\",\n",
    "\"publisher\":\"\"\"</publisher>\"\"\",\n",
    "\"volume\":\"\"\"</biblScope>\"\"\",\n",
    "\"year\":\"\"\"</date>\"\"\",\n",
    "\"address\":\"\"\"</pubPlace>\"\"\",\n",
    "\"number\":\"\"\"</biblScope>\"\"\",\n",
    "\"pages\":\"\"\"</biblScope>\"\"\",\n",
    "\"pagetotal\":\"\"\"</biblScope>\"\"\"}\n",
    "\n",
    "\n",
    "\n",
    "begin_file = \"\"\"<?xml version=\"1.0\" ?>\n",
    "<TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" \n",
    " xmlns:mml=\"http://www.w3.org/1998/Math/MathML\">\n",
    "\t<teiHeader>\n",
    "\t\t<fileDesc xml:id=\"0\"/>\n",
    "\t</teiHeader>\n",
    "\t<text>\n",
    "\t\t<front/>\n",
    "\t\t<body/>\n",
    "\t\t<back>\n",
    "<listBibl>\"\"\"\n",
    "end_file = \"\"\"\t\t</listBibl>\n",
    "\t</back>\n",
    "\t</text>\n",
    "</TEI>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tei_ref = []\n",
    "for ref_text, ref_labels in zip(test_text,test_labels):\n",
    "    new_ref_text = \"\"\n",
    "    for ref_token, label in zip(ref_text.split(\" \"),ref_labels.split(\" \")):\n",
    "        if label == \"other\":\n",
    "            new_ref_text+=ref_token + \" \"\n",
    "        else:\n",
    "            new_ref_text+=label_dict_beg[label]+ref_token+label_dict_end[label]+ \" \"\n",
    "    new_ref_text = new_ref_text.replace(\"<i/>\",\"\").replace(\"\\x16\",\"--\").replace(\"< i / >\",\"\").replace(\"\"\"< \"\"\",\"\"\"&gt; \"\"\").replace(\"\"\" >\"\"\",\"\"\" &lt;\"\"\") # Anpassungen bewerten..\n",
    "    new_ref_text = new_ref_text.replace(\" &\",\"&amp; \").replace(\"& \",\"&amp; \")\n",
    "    tei_ref.append(new_ref_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_output = \"C:/Masterarbeit/Data/manuel_annotated/grobid_evaluation_token_level\"\n",
    "path_output = \".\\Sequence-Labeling-for-Reference-Parsing-of-Cyrillic-Script-Scholarly-Data\\Grobid\\grobid_test_data\"\n",
    "\n",
    "for r,f in zip(tei_ref,file_ids):\n",
    "    output_text = begin_file + \"\\n<bibl>\" +r+\"</bibl>\\n\" +end_file\n",
    "    myfile = open(os.path.join(path_output, f+\".references.tei.xml\"), \"w\",encoding=\"utf-8\")\n",
    "    myfile.write(output_text)\n",
    "    myfile.close()\n",
    "print(\"DONE\")\n",
    "print(str(len(tei_ref))+ \" files created!\")\n",
    "\n",
    "files_grobid = listdir_path(path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually look into error files and remove errors!\n",
    "errors = []\n",
    "for f in files_grobid:\n",
    "    try:\n",
    "        tree = ET.parse(f)\n",
    "        root = tree.getroot()\n",
    "    except:\n",
    "        \n",
    "        errors.append(f)\n",
    "#         os.remove(f)\n",
    "print(\"errors in:  \" + str(len(errors)) + \" files. Files removed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
