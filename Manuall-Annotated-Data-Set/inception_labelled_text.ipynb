{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform \"XML 1.1\" to \"Labelled Text\"\n",
    "\n",
    "There are two steps to achieve this:\n",
    "\n",
    "* Extract data from the XML 1.1 Files\n",
    "* Paste the reference strings & create labelled strings\n",
    "* Write this data in a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages & define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element, SubElement, Comment, tostring\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "import os\n",
    "import fasttext\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_path(d):\n",
    "    # Return full path of all files & directories in directory\n",
    "    list_full_path = []\n",
    "    for path in os.listdir(d):\n",
    "        full_path = os.path.join(d, path)\n",
    "        list_full_path.append(full_path)\n",
    "    return list_full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = \".\\Sequence-Labeling-for-Reference-Parsing-of-Cyrillic-Script-Scholarly-Data\\Real_annotated_data\\manual_anno_xml1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dir_ = \"D:/manual_anno\"\n",
    "files = [f for f in listdir_path(dir_)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # get the xml1.1 file\n",
    "    # tree = ET.parse(\"C:\\\\Masterarbeit\\\\Data\\\\manuel_annotated\\\\XML 1.1\\\\annotation\\\\Core_ID_11312400.pdf\\\\ulede.xmi\")\n",
    "    core_id = file.split(\"\\\\\")[1].split(\".\")[0][8:]\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    tokens_doc = []\n",
    "    labels_doc = []\n",
    "    ref_span = []\n",
    "    count_ref = 0 #ref no. ended\n",
    "    ref = 0 #ref no. started\n",
    "    d = {\"Token\" : [] , \"Label\" : []}\n",
    "    for lt in root.findall(\"{http:///webanno/custom.ecore}Label\"):\n",
    "        label = lt.attrib[\"value\"]\n",
    "        if label ==\"ref_beg\" and ref == count_ref :\n",
    "            count_ref+=1\n",
    "            beg = int(lt.attrib[\"end\"])\n",
    "    #         print(label + \" :  ref =  \" + str(ref)+ \"  count_ref = \" +str(count_ref))\n",
    "\n",
    "        elif label == \"ref_end\":\n",
    "            end = int(lt.attrib[\"end\"])\n",
    "            ref +=1\n",
    "            span = [beg,end]\n",
    "            ref_span.append(span)\n",
    "            span = []\n",
    "    #         print(label + \" :  ref =  \" + str(ref)+ \"  count_ref = \" +str(count_ref))\n",
    "        elif label==\"ref_beg\" and ref!=count_ref:\n",
    "            end = int(lt.attrib[\"begin\"])\n",
    "            ref +=1\n",
    "            count_ref+=1\n",
    "            span = [beg,end]\n",
    "            ref_span.append(span)\n",
    "            span = []\n",
    "            beg = int(lt.attrib[\"end\"])\n",
    "    #         print(label + \" :  ref =  \" + str(ref)+ \"  count_ref = \" +str(count_ref))\n",
    "\n",
    "    references_plain = []\n",
    "    for span in ref_span:\n",
    "        references_plain.append(root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].attrib[\"sofaString\"][span[0] : span[1]])\n",
    "        \n",
    "    labelled_text = []\n",
    "    for span , text in zip(ref_span,references_plain):\n",
    "        add_len = 0\n",
    "        for lt in root.findall(\"{http:///webanno/custom.ecore}Label\"):\n",
    "            label = lt.attrib[\"value\"]\n",
    "            token = root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].attrib[\"sofaString\"][int(lt.attrib[\"begin\"]) : int(lt.attrib[\"end\"])]\n",
    "\n",
    "            if (label not in (\"ref_beg\",\"ref_end\")) and (int(lt.attrib[\"end\"]) in range(span[0],span[1]+1)):\n",
    "                beg = int(lt.attrib[\"begin\"]) - span[0] + add_len\n",
    "                end = int(lt.attrib[\"end\"]) - span[0] + add_len\n",
    "#                 if re.match(\".* $\",token)!=None:\n",
    "                if token[-1]==\" \":\n",
    "#                     if label == \"issue\":\n",
    "#                         new_text = text[0:beg] + \"<number>\" + token[:-1] + \"</number> \" + text[end:]\n",
    "#                     else:\n",
    "                    new_text = text[0:beg] + f\"<{label}>\" + token[:-1] + f\"</{label}> \" + text[end:]\n",
    "                else:\n",
    "#                     if label == \"issue\":\n",
    "#                         new_text = text[0:beg] + \"<number>\" + token + \"</number> \" + text[end:]\n",
    "#                     else:\n",
    "                    new_text = text[0:beg] + f\"<{label}>\" + token + f\"</{label}>\" + text[end:]\n",
    "                text = new_text\n",
    "    #             print(text)\n",
    "#                 if label == \"issue\":\n",
    "#                     add_len += len(\"<number>\" + \"\" + \"</number>\")\n",
    "#                 else:\n",
    "                add_len += len(f\"<{label}>\" + \"\" + f\"</{label}>\")\n",
    "#         labelled_text.append(text.replace(\"-\\n\",\"\").replace(\"\\n\",\"\"))\n",
    "        text = text.replace(\"<issue>\",\"<number>\").replace(\"</issue>\",\"</number>\")\n",
    "        text = text.replace(\"<idno_other>\",\"\").replace(\"</idno_other>\",\"\").replace(\"<idno_doi>\",\"\").replace(\"</idno_doi>\",\"\")\n",
    "        labelled_text.append(\" \".join(re.sub(r\"-\\n\\s+\", \"\", text).replace(\"-\\n\",\"\").replace(\"\\n\",\"\").split()).replace(\"\"\" </\"\"\",\"\"\"</\"\"\"))\n",
    "    if \"<volumen>\" in str(labelled_text) or \"<jo#>\" in str(labelled_text):\n",
    "        print(core_id)\n",
    "    mydata = \"\\n\\n\".join(labelled_text)\n",
    "#     myfile = open(os.path.join(\"C:\\Masterarbeit\\Data\\manuel_annotated\\labelled_text\",core_id)+\".txt\", \"w\",encoding=\"utf-8\")\n",
    "    myfile = open(os.path.join(\".\\Sequence-Labeling-for-Reference-Parsing-of-Cyrillic-Script-Scholarly-Data\\Real_annotated_data\\labelled_text_per_paper\",core_id)+\".txt\", \"w\",encoding=\"utf-8\")\n",
    "\n",
    "    myfile.write(mydata)       \n",
    "    myfile.close()\n",
    "#     print(labelled_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6-64-tensorflow1.11",
   "language": "python",
   "name": "python3.6-64-tensorflow1.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
