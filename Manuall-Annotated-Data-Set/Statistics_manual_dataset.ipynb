{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "# import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_json(\"D:/NAACL/348.json\" , lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second json with data\n",
    "df_data2 = pd.read_json(\"D:/NAACL/core_all_cyr.jsonl\" , lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_data.append(pd.DataFrame(data = df_data2), ignore_index=True)\n",
    "df_full = df_full.drop_duplicates(subset=[\"coreId\"],keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the final PDF Core_ids\n",
    "files = os.listdir(\"C:/Masterarbeit/Data/manuel_annotated/labelled_text\")\n",
    "# f in files re.sub(\"Core_ID_\",\"\",files)\n",
    "core_ids= [int(re.sub(\".txt\",\"\",f)) for f in files]\n",
    "len(core_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full[df_full.coreId.isin(core_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.reset_index(drop = True , inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = []\n",
    "for i in df_full.relations:\n",
    "    try:\n",
    "        string.append(i[0])\n",
    "    except:\n",
    "        string.append(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.relations = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = [re.sub(\"[0-9]\", \"\",x) for x in df_full.relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of year distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_year = [df_full.year[df_full.coreId.isin(core_ids)].iloc[i] for i in range(len(core_ids))]\n",
    "df_year = df_full.year[df_full.coreId.isin(core_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(vec_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of years\n",
    "len(list(set(vec_year)))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# gaussian_numbers = np.random.normal(size=10000)\n",
    "plt.hist(vec_year, bins=10)\n",
    "plt.title(\"Gaussian Histogram of publication year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Frequency of papers\")\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('publication_year.pdf'  ,  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for topic in list(df_full.topics):\n",
    "    try:\n",
    "        if re.match(\"([a-zA-Z]|[А-Яа-я])\",topic[0]):\n",
    "            top = re.sub(\"Выпуск \\d*. Серия: \",\"\",topic[0])\n",
    "            top = ''.join([i for i in top if not i.isdigit()])\n",
    "            top = top.replace(\"\"\"(\"\"\",\"\").replace(\"\"\")\"\"\",\"\")\n",
    "            if re.search(\"(коммуналь)|(город)\",top.lower())!=None:\n",
    "                topics.append(\"Urban Planning/Infrastructure\")\n",
    "            elif re.search(\"(эконом)|(економічні)\",top.lower())!=None:\n",
    "                topics.append(\"Economics\")\n",
    "            elif re.search(\"(технические науки)|(технологии)|(технічні)\",top.lower())!=None:\n",
    "                topics.append(\"Technics\")\n",
    "            elif re.search(\"(education)|(воспитания)\",top.lower())!=None:\n",
    "                topics.append(\"Education\")\n",
    "            elif re.search(\"(электроэнергетик)|(светотехник)\",top.lower())!=None:\n",
    "#                 topics.append(\"(электроэнергетик)|(светотехник)\")\n",
    "                topics.append(\"Technics\")\n",
    "            else:\n",
    "                topics.append(top)\n",
    "        else:\n",
    "             topics.append(\"no topic\")\n",
    "             pass\n",
    "    except:\n",
    "         topics.append(\"no topic\")\n",
    "         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "x =Counter(topics)\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for labelled reference strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final PDF Core_ids\n",
    "files = os.listdir(\"C:/Masterarbeit/Data/manuel_annotated/labelled_text\")\n",
    "# f in files re.sub(\"Core_ID_\",\"\",files)\n",
    "core_ids= [int(re.sub(\".txt\",\"\",f)) for f in files]\n",
    "len(core_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ref = []\n",
    "all_ref =[]\n",
    "for f in files:\n",
    "    file = open(os.path.join(\"C:/Masterarbeit/Data/manuel_annotated/labelled_text\",f),\"r\",encoding=\"utf-8\")\n",
    "    ref_list = file.read().split(\"\\n\\n\")\n",
    "    all_ref+=ref_list\n",
    "    count_ref.append(len(ref_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Anzahl manuell gelabelter Referenzen\n",
    "sum(count_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "label_list= []\n",
    "all_labels = []\n",
    "for ref in all_ref:\n",
    "    labels = []\n",
    "    for t in ref.split(\"\"\">\"\"\"):\n",
    "#         print(t)\n",
    "        if \"</\" not in t and \"\"\"<\"\"\" in t:\n",
    "            labels.append(t.split(\"<\")[1])\n",
    "#     print(labels)    \n",
    "    label_list.append(labels)\n",
    "    all_labels += labels\n",
    "count_all_labels = Counter(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all_labels.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum of labels\n",
    "i=0\n",
    "for count in count_all_labels.most_common():\n",
    "    i+=count[1]\n",
    "    \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ref_len = []\n",
    "all_ref_clean = []\n",
    "## Average reference length (in tokens)\n",
    "for text in all_ref:\n",
    "\n",
    "    for label in set(all_labels):\n",
    "        text = re.sub(f\"(\\<{label}\\>)|(\\</{label}\\>)\",\"\",text)\n",
    "    ref_len.append(len(nltk.word_tokenize(text,preserve_line=True)))\n",
    "    all_ref_clean.append(text)\n",
    "sum(ref_len)/len(ref_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper language detection\n",
    "path_to_txt = \"D:/NAACL/Parsed_Text_15553/TXTs_15553\"\n",
    "paper_lang = []\n",
    "for core_id in core_ids:\n",
    "    file = open(os.path.join(path_to_txt,\"Core_ID_\"+str(core_id)+\".pdf.txt\"),\"r\",encoding=\"utf-8\")\n",
    "    file = file.read().replace(\"\\n\",\"\")\n",
    "#     print(file)\n",
    "    predictions = fasttext_detection_model.predict(file)\n",
    "    paper_lang.append(predictions[0][0])\n",
    "Counter(paper_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_ref_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference language detection\n",
    "\n",
    "path_to_txt = \"D:/NAACL/Parsed_Text_15553/TXTs_15553\"\n",
    "ref_lang = []\n",
    "for ref in all_ref_clean:\n",
    "    ref = ref.replace(\"\\n\",\"\")\n",
    "\n",
    "    predictions = fasttext_detection_model.predict(ref)\n",
    "    ref_lang.append(predictions[0][0])\n",
    "ref_lang_count = Counter(ref_lang)\n",
    "ref_lang_count.most_common()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6-64-tensorflow1.11",
   "language": "python",
   "name": "python3.6-64-tensorflow1.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
