{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "import os\n",
    "# PdfMiner\n",
    "import glob\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "# BibTex\n",
    "import bibtexparser\n",
    "# Regex\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Copy Files\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePassage(my_str):\n",
    "    my_str = re.sub(\"\\\\\\\\ud\", \" \", my_str)\n",
    "    my_str = re.sub(\"\\n\", \" \", my_str)\n",
    "#     my_str = re.sub(\":\", \" \", my_str)\n",
    "#     my_str5 = re.sub(\"\\(|\\)\", \" \", my_str3)\n",
    "    my_str = re.sub(\"  \", \" \", my_str)\n",
    "    return(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_subset(range1, range2):\n",
    "    \"\"\"Whether range1 is a subset of range2.\"\"\"\n",
    "    if not range1:\n",
    "        return True  # empty range is subset of anything\n",
    "    if not range2:\n",
    "        return False  # non-empty range can't be subset of empty range\n",
    "    if len(range1) > 1 and range1.step % range2.step:\n",
    "        return False  # must have a single value or integer multiple step\n",
    "    return range1.start in range2 and range1[-1] in range2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funktion zum parsen von PDF zu String Format [Von selbst erstellten PDFs!]\n",
    "\n",
    "def extractor(path):\n",
    "    output_string = StringIO()\n",
    "    with open(path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "        return(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nltk_token_text(string , preserve_line):\n",
    "#     tokens = nltk.word_tokenize(string.lower(),language = 'russian',preserve_line=preserve_line)\n",
    "#     text = str(' '.join(tokens))\n",
    "#     return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_token_text(string , preserve_line):\n",
    "    pattern = re.compile(\"[0-9]+\\.\")\n",
    "    pattern2= re.compile(\":[0-9]+\")\n",
    "    pattern3= re.compile(\"([A-Z]\\.)|([\\wа-я]\\.)\")\n",
    "    pattern4 = re.compile(\".*\\.\")\n",
    "\n",
    "    tokens = nltk.word_tokenize(string.lower(),language = 'russian',preserve_line=preserve_line)\n",
    "    nltk_tokens = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        if pattern2.match(t)!=None:\n",
    "            nltk_tokens.append(\":\")\n",
    "            if \".\" in t:\n",
    "                nltk_tokens.append(t.replace(\":\",\"\").replace(\".\",\"\"))\n",
    "            else:\n",
    "                nltk_tokens.append(t.replace(\":\",\"\"))\n",
    "            nltk_tokens.append(\".\")\n",
    "        elif ((pattern.match(t)!= None) or (pattern3.match(t)== None and pattern4.match(t)!=None)) and t !=\".\":\n",
    "            nltk_tokens.append(t.replace(\".\",\"\"))\n",
    "            nltk_tokens.append(\".\")\n",
    "\n",
    "        else:\n",
    "            nltk_tokens.append(t)\n",
    "        \n",
    "    text = str(' '.join(nltk_tokens))\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return \"The strings are {} edits away\".format(distance[row][col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Bib entries with PDF Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOST2006 style\n",
    "[1] Author. Title [Text] / Author i Author // Journal. -- year. -- No.volume(number). -- C. pages.\n",
    "\n",
    "In dieser Reihenfolge gehe ich die Labels durch und checke, ob es zu Überschneidungen kommt. Wenn Ja, dann suche nächsten match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_input = \"C:\\\\Masterarbeit\\\\venvPDF\\\\pdf_bib\\\\gost2006\"\n",
    "# path_output = \"C:\\\\Masterarbeit\\\\venvPDF\\\\labelled_text\\\\gost2006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = \".\\Sequence-Labeling-for-Reference-Parsing-of-Cyrillic-Script-Scholarly-Data\\Synthetic_data\\pdf_bibtex_data\\clean_gost2006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f.replace(\".pdf\",\"\") for f in listdir(path_input) if isfile(join(path_input, f))]\n",
    "\n",
    "counterfiles = Counter(onlyfiles)\n",
    "\n",
    "pdf_list = []\n",
    "for c in counterfiles.keys():\n",
    "    if counterfiles[c]>1:\n",
    "        pdf_list.append(c)\n",
    "len(pdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test readability of papers\n",
    "not_readable_pdf = []\n",
    "for bib in pdf_list:\n",
    "    try:\n",
    "        pdf = extractor(path_input+\"\\\\\"+bib+\".pdf\")\n",
    "        text = pdf.getvalue()\n",
    "#         text.encode('latin').decode('windows-1251')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        not_readable_pdf.append(bib)\n",
    "        pass\n",
    "    \n",
    "# Test readability of bibtex\n",
    "not_readable_bib = []\n",
    "for bib in pdf_list:\n",
    "    try:\n",
    "        with open(path_input + \"\\\\\" +bib , encoding = 'utf-8') as bibtex_file:\n",
    "            bibtex_str = bibtex_file.read()\n",
    "\n",
    "        bib_database = bibtexparser.loads(bibtex_str)\n",
    "\n",
    "\n",
    "        for key in bib_database.entries[0].keys():\n",
    "            if key!='author':\n",
    "                globals()[key] = nltk_token_text(removePassage(bib_database.entries[0][key]),False)#.replace(\".\",\"\").replace(\",\",\"\")\n",
    "            else:\n",
    "                globals()[key] = removePassage(bib_database.entries[0][key])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        not_readable_bib.append(bib)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-readable PDFs & Bibtex\n",
    "i=0\n",
    "for p in pdf_list:\n",
    "    if (p in not_readable_pdf) or (p in not_readable_bib):\n",
    "        pdf_list.remove(p)\n",
    "        i+=1\n",
    "        \n",
    "print(f\"{i} PDF´s Removed! \\n Current size of dataset {len(pdf_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy clean files to new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dieser Schritt kann ausgelassen werden, da ich schon \"saubere\" Daten liefere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy \"clean\" files to new directory    copyfile(path_input+\"\\\\\"+p, \"C:\\\\Masterarbeit\\\\venvPDF\\\\clean_plain_data\\\\\"+p)\n",
    "path = \"C:\\\\Masterarbeit\\\\venvPDF\\\\pdf_bib\\\\clean_gost2006\\\\\"\n",
    "for p in pdf_list:\n",
    "    copyfile(path_input+\"\\\\\"+p, path+p)\n",
    "    copyfile(path_input+\"\\\\\"+p+'.pdf', path+p+'.pdf')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labelled text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_input = \"C:\\\\Masterarbeit\\\\venvPDF\\\\pdf_bib\\\\clean_gost2006\"\n",
    "# path_output = \"C:\\\\Masterarbeit\\\\venvPDF\\\\labelled_text\\\\gost2006_fine_grained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = \".\\Sequence-Labeling-for-Reference-Parsing-of-Cyrillic-Script-Scholarly-Data\\Synthetic_data\\pdf_bibtex_data\\clean_gost2006\"\n",
    "path_output = \".\\Sequence-Labeling-for-Reference-Parsing-of-Cyrillic-Script-Scholarly-Data\\Synthetic_data\\Labeled_text_data\\gost2006_fine_grained_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f.replace(\".pdf\",\"\") for f in listdir(path_input) if isfile(join(path_input, f))]\n",
    "\n",
    "counterfiles = Counter(onlyfiles)\n",
    "\n",
    "pdf_list = []\n",
    "for c in counterfiles.keys():\n",
    "    if counterfiles[c]>1:\n",
    "        pdf_list.append(c)\n",
    "len(pdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "iter_count = 0\n",
    "for bib in pdf_list:\n",
    "    iter_count += 1\n",
    "    pdf = extractor(path_input+\"\\\\\"+bib+\".pdf\")\n",
    "\n",
    "    ### Read PDF Text\n",
    "    pdf_text = pdf.getvalue().replace(\"(cid:22)\",\"\u0016\")#.encode('latin').decode('windows-1251')\n",
    "    # 1.) Wenn Wort nicht in eine Zeile passt und aufgesplittet wird, wollen wir das wort wieder zsm führen\n",
    "    # 2.) \"-\" wird als \\x15 ausgegeben\n",
    "    # 3.) \"-\" wird als \\x15 ausgegeben\n",
    "    # 4.) \"-\" wird als \\x15 ausgegeben'\n",
    "#     pdf_text2 = pdf_text.replace(\"-\\n\",\"\").replace(\"\\x15\",\"-\").replace(\"\\x10\",\"“\").replace(\"\\x11\",\"”\").replace(\"'\",\"’\")\n",
    "    pdf_text2 = pdf_text.replace(\"-\\n\",\"\").replace(\"\\x15\",\"-\").replace(\"\\x10\",\"“\").replace(\"\\x11\",\"”\").replace(\"'\",\"’\").replace(\"–\\n\",\"–\")\n",
    "\n",
    "    pdf_text2 = removePassage(pdf_text2)\n",
    "    pdf_text2 = pdf_text2.replace(\"Список литературы [1]\",\"\")\n",
    "    \n",
    "    ### Transform & Clean with nltk\n",
    "    nltk_tokens = []\n",
    "    pattern = re.compile(\"[0-9]+\\.\")\n",
    "    pattern2= re.compile(\":[0-9]+\")\n",
    "    pattern3= re.compile(\"([A-Z]\\.)|([\\wа-я]\\.)\")\n",
    "    pattern4 = re.compile(\".*\\.\")\n",
    "\n",
    "    for t in nltk.word_tokenize(pdf_text2,preserve_line=True):\n",
    "        if pattern2.match(t)!=None:\n",
    "            nltk_tokens.append(\":\")\n",
    "            if \".\" in t:\n",
    "                nltk_tokens.append(t.replace(\":\",\"\").replace(\".\",\"\"))\n",
    "            else:\n",
    "                nltk_tokens.append(t.replace(\":\",\"\"))\n",
    "            nltk_tokens.append(\".\")\n",
    "        elif ((pattern.match(t)!= None) or (pattern3.match(t)== None and pattern4.match(t)!=None)) and t !=\".\":\n",
    "            nltk_tokens.append(t.replace(\".\",\"\"))\n",
    "            nltk_tokens.append(\".\")\n",
    "\n",
    "        else:\n",
    "            nltk_tokens.append(t)\n",
    "\n",
    "    nltk_text = str(' '.join(nltk_tokens))\n",
    "\n",
    "    ### Get BibTeX data\n",
    "    with open(path_input + \"\\\\\" +bib , encoding = 'utf-8') as bibtex_file:\n",
    "        bibtex_str = bibtex_file.read()\n",
    "\n",
    "    bib_database = bibtexparser.loads(bibtex_str)\n",
    "\n",
    "\n",
    "    for key in bib_database.entries[0].keys():\n",
    "        if key!='author':\n",
    "            if key == \"pages\":\n",
    "                globals()[key] = nltk_token_text(removePassage(bib_database.entries[0][key]),False).replace(\" -- \",\"–\")\n",
    "            elif (key == \"address\") and (re.match(\".\\.\",bib_database.entries[0][key])):\n",
    "                globals()[key] = removePassage(bib_database.entries[0][key])\n",
    "            else:\n",
    "                globals()[key] = nltk_token_text(removePassage(bib_database.entries[0][key]),False).replace(\"\\\\textit { \",\"\").replace(\"\\\\textsubscript { \",\"\").replace(\"\\\\textsuperscript { \",\"\").replace(\"\"\" } \"\"\",\"\")\n",
    "        else:\n",
    "            globals()[key] = removePassage(bib_database.entries[0][key])\n",
    "\n",
    "    ### Split Author Entries\n",
    "    author2 = \" \".join(author.split(\" and\")).split(\"  \")\n",
    "    list_authors = []\n",
    "    for a in author2:\n",
    "        a_list = a.split(\", \")\n",
    "        list_authors.append(a_list)\n",
    "\n",
    "    ##########################################################\n",
    "    ### Match Bib Entries with PDF Text\n",
    "    ##########################################################\n",
    "    # OHNE JOURNAL & AUTHOR!\n",
    "    if bib_database.entries[0][\"type\"] == \"Journal Article\":\n",
    "        LABELS_TXT = [\"year\",\"title\",\"journal\",\"volume\",\"number\",\"pages\",\"pagetotal\"] # Ohne ,\"author\" da dieses Label aus einer Liste besteht!\n",
    "    else:\n",
    "        LABELS_TXT = [\"year\",\"title\",\"booktitle\",\"volume\",\"number\",\"pages\",\"address\",\"publisher\",\"pagetotal\"]\n",
    "        \n",
    "    LABELS_VAR = [] # Ohne author da dieses Label aus einer Liste besteht!\n",
    "    LABELS_TXT2 = []\n",
    "    for l in LABELS_TXT:\n",
    "        if l in bib_database.entries[0].keys():\n",
    "            LABELS_TXT2.append(l)\n",
    "            LABELS_VAR.append(globals()[l])\n",
    "    LABELS_TXT = LABELS_TXT2\n",
    "    # Wie damit umgehen, wenn Werte mehrfach vorkommen??? z.B. V.V. bei Autoren\n",
    "    token_spans = pd.DataFrame(columns=['token','start',\"end\",\"label\"])\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for token in nltk_tokens:\n",
    "    #     print(token)\n",
    "        length = len(token)\n",
    "        end += length\n",
    "        d = {'token':[token], 'start':[start],'end':[end], 'label':None}\n",
    "        token_spans = token_spans.append(pd.DataFrame(d),ignore_index = True)\n",
    "        start+=length +1\n",
    "        end+=1\n",
    "\n",
    "    label_spans = pd.DataFrame(columns=['text','start',\"end\",\"label\"])\n",
    "    # 22 in der find function, weil erst ab Zeichen 22 die Referenz beginnt\n",
    "    # Suche nach Author Einträgen im Text und hole die spans\n",
    "    for full_author in list_authors:\n",
    "        for text in full_author:\n",
    "            try:\n",
    "                start_found = [m.start() for m in re.finditer(text.lower(), nltk_text.lower())]\n",
    "                for s in start_found:\n",
    "                    span_label =[s ,s+len(text)]\n",
    "                    i=0\n",
    "                    while any([range_subset(range(span_label[0],span_label[1]),range(start,end)) for start,end in zip(label_spans.start,label_spans.end)])==True:\n",
    "                        span_label =[nltk_text.lower().find(text.lower(),span_label[0]+1) ,nltk_text.lower().find(text.lower(),span_label[0]+1)+len(text)]\n",
    "                        i+=1\n",
    "                        if i==10:\n",
    "                            break\n",
    "\n",
    "                    d = {'text':[text], 'start':[span_label[0]], 'end':[span_label[1]], 'label':['author']}\n",
    "                    label_spans = label_spans.append(pd.DataFrame(d),ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Suche nach Labels Einträgen im Text und hole die spans [ausser Author]\n",
    "    missing_labels = []\n",
    "    for text,label in zip(LABELS_VAR,LABELS_TXT):\n",
    "\n",
    "        text2 = \" \"+text.lower()+\" \"\n",
    "        try:\n",
    "            span_label = [nltk_text.lower().find(text2)+1 ,nltk_text.lower().find(text2)+len(text)+1]\n",
    "            i=0\n",
    "            while any([range_subset(range(span_label[0],span_label[1]),range(start,end)) for start,end in zip(label_spans.start,label_spans.end)])==True:\n",
    "                span_label =[nltk_text.lower().find(text2,span_label[0])+1 ,nltk_text.lower().find(text2,span_label[0])+len(text)+1]\n",
    "                i+=1\n",
    "                if i==10:\n",
    "                    break\n",
    "            if span_label[0] <1: #Wenn z.B. Titel nicht mit dem Titel im Text übereinstimmt (ein Zeichen falsch intepretiert), wird ein komischer span ausgegeben z.B. [-1,107]. Daher labeln wir den Titel lieber nicht \n",
    "                missing_labels.append(label)\n",
    "                pass\n",
    "            else:\n",
    "                d = {'text':[text], 'start':[span_label[0]], 'end':[span_label[1]], 'label':[label]}\n",
    "                label_spans = label_spans.append(pd.DataFrame(d),ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for token_start,token_end in zip(token_spans.start, token_spans.end): #Für jeden Eintrag aus token_spans ...\n",
    "        for label_start, label_end, label in zip(label_spans.start,label_spans.end,label_spans.label): # ... suche nach Match in label_spans\n",
    "            if range_subset(range(token_start,token_end),range(label_start,label_end)):\n",
    "                token_spans.label[(token_spans.start == token_start)&(token_spans.end == token_end)] = label\n",
    "    \n",
    "    #Levenshtein distance\n",
    "    for text,label in zip(LABELS_VAR,LABELS_TXT):\n",
    "        if label in missing_labels:\n",
    "            ### Window function\n",
    "            none_tokens = token_spans.token[token_spans.label.isin([None])]\n",
    "            window_size = len(text.split())\n",
    "            indexes = list(none_tokens.index)\n",
    "            all_dist = []\n",
    "            all_windows = []\n",
    "            for i in none_tokens.rolling(window=window_size):\n",
    "    #             print(' '.join(i))\n",
    "                distance = levenshtein_ratio_and_distance(' '.join(i).lower() , text,True)\n",
    "                all_dist.append(distance)\n",
    "                all_windows.append(i)\n",
    "            max_match = all_dist.index(max(all_dist))\n",
    "            indizes = list(all_windows[max_match].index)\n",
    "            if indizes != list(range(indizes[0], indizes[-1]+1)):\n",
    "                i_ = 0\n",
    "                for idz in list(range(indizes[0], indizes[-1]+1)):\n",
    "                    if idz not in indizes:\n",
    "                        indizes2 = indizes[i_:]\n",
    "                    i_ =+1\n",
    "            else:\n",
    "                indizes2 = indizes\n",
    "            if token_spans.token[indizes2[0]] == \".\":\n",
    "                indizes2 = indizes2[1:]\n",
    "            for i in indizes2:\n",
    "                token_spans['label'][i:i+1] = str(label)\n",
    "                \n",
    "                \n",
    "    if \"number\" not in token_spans.label:\n",
    "        try:\n",
    "            if token_spans.token[list(token_spans[token_spans.label.isin(['volume'])].index + 2)].iloc[0] == number:\n",
    "                token_spans.label[list(token_spans[token_spans.label.isin(['volume'])].index + 2)]=\"number\"\n",
    "        except:\n",
    "            pass\n",
    "### Create Labelled Text\n",
    "    LABELS_TXT.append(\"author\")\n",
    "\n",
    "    output_text_token = []\n",
    "    for token, label in zip(token_spans.token,token_spans.label):\n",
    "        if label != None:\n",
    "#             if label == \"booktitle\":\n",
    "#                 output_text_token.append(\"<journal>\" + token + \"</journal>\")\n",
    "#             elif label == \"pagetotal\":\n",
    "#                 output_text_token.append(\"<pages>\" + token + \"</pages>\")\n",
    "#             else:\n",
    "            output_text_token.append(f\"<{label}>\" + token + f\"</{label}>\")\n",
    "        else:\n",
    "            output_text_token.append(token)\n",
    "\n",
    "    output_text = ' '.join(output_text_token)  \n",
    "    myfile = open(path_output + \"\\\\\"+bib.replace(\".bib\",\".xml\"), \"w\",encoding=\"utf-8\")\n",
    "    myfile.write(output_text)\n",
    "    myfile.close()\n",
    "    if iter_count % 500 == 0:\n",
    "        print(iter_count)\n",
    "        \n",
    "end_time = datetime.now()\n",
    "print(difference = start_time - end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPDF",
   "language": "python",
   "name": "venvpdf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
